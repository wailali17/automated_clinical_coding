# Automated Clinical Coding with Large Language Models

## Overview

This repository explores the use of traditional machine learning models and large language models (LLMs) to automate clinical coding. Using the MIMIC-IV dataset, the project evaluates the accuracy, interpretability, and efficiency of LLMs compared to baseline models. The repository includes scripts for data preparation, model training, and evaluation, as well as exploration notebooks.

## Prerequisites
1. Install Required Packages:
- Use the provided `requirements.txt` to install the necessary Python dependencies.

    ``` pip install -r requirements.txt ```

2. Set Up OpenAI API:
- Create a `.env` file in the root directory and add your OpenAI API key:

    ``` OPENAI_API_KEY = your_openai_api_key ```

3. Download Dataset:
- Obtain the MIMIC-IV dataset from the [PhysoNet](https://physionet.org/content/mimiciv/2.2/) website. Ensure the dataset is properly formatted and saved in the `original_data/` directory.

## Project Directory Structure

```
ðŸ“‚ AUTOMATED_CLINICAL_CODING
â”œâ”€â”€ ðŸ“‚ code_utils
â”‚   â”œâ”€â”€ ðŸ“„ baseline_coder.py         # Implements baseline models (TF-IDF, PCA)
â”‚   â”œâ”€â”€ ðŸ“„ data_prep.py              # Prepares and merges datasets
â”‚   â”œâ”€â”€ ðŸ“„ llm_coder.py              # Predicts ICD codes using LLMs
â”‚   â”œâ”€â”€ ðŸ“„ openai_vectorise_llm.py   # Embeds textual features using OpenAI embeddings
â”‚   â””â”€â”€ ðŸ“„ utils.py                  # Helper functions for evaluation and utilities
â”‚
â”œâ”€â”€ ðŸ“‚ data
â”‚   â”œâ”€â”€ ðŸ“‚ original_data             # Raw MIMIC-IV dataset
â”‚   â”œâ”€â”€ ðŸ“‚ altered_data              # Preprocessed and intermediate data
â”‚   â”œâ”€â”€ ðŸ“‚ embeddings                # Embedding files for textual features
â”‚   â””â”€â”€ ðŸ“‚ eval                      # Evaluation metrics and outputs
|   â””â”€â”€ ðŸ“‚ definitions               # JSON and text files
|   â””â”€â”€ ðŸ“‚ logging                   # Tracking events that occur in code run
|   â””â”€â”€ ðŸ“‚ models                    # Stored pickle files for XGBoost models
â”‚
â”œâ”€â”€ ðŸ“‚ notebooks
â”‚   â”œâ”€â”€ ðŸ“„ eda.ipynb                 # Exploratory Data Analysis
â”‚   â””â”€â”€ ðŸ“„ evaluation.ipynb          # Evaluating models and visualizations
â”‚
â”œâ”€â”€ ðŸ“„ main.py                       # Main script orchestrating the entire workflow
â”œâ”€â”€ ðŸ“„ requirements.txt              # List of Python dependencies
â””â”€â”€ ðŸ“„ README.md                     # Project documentation
â””â”€â”€ ðŸ“„ .env                          # File containing secret keys
```

## Running the Project

Execute the `main.py` script in a command window and this should run everything from data prepartion to running the LLM model. Ensure that you have replicated the above directory structure and followed instructions in the prerequisites.

## Key Features
- **Data Preparation**: Combines multiple datasets (admissions, diagnoses, lab events) and preprocesses them for modeling.
- **Baseline Models**: Implements TF-IDF vectorization and PCA for traditional machine learning.
- **LLM Integration**: Uses OpenAI embeddings and GPT models to leverage advanced language understanding.
- **Evaluation**: Provides detailed metrics and visualisations for model comparison.